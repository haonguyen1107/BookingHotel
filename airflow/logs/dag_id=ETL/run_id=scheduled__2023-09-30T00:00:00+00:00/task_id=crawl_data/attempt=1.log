[2023-09-30T12:30:54.069+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T12:30:54.074+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T12:30:54.079+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T12:30:54.079+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T12:30:54.092+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T12:30:54.096+0000] {standard_task_runner.py:57} INFO - Started process 65 to run task
[2023-09-30T12:30:54.099+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmp64qhl7qv']
[2023-09-30T12:30:54.099+0000] {standard_task_runner.py:85} INFO - Job 7: Subtask crawl_data
[2023-09-30T12:30:54.109+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T12:30:54.130+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host 64844768c165
[2023-09-30T12:30:54.137+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T12:30:54.176+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T12:30:54.177+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 19, in extract_data
    booking_links = config['booking_links']
KeyError: 'booking_links'
[2023-09-30T12:30:54.189+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T123054, end_date=20230930T123054
[2023-09-30T12:30:54.196+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 7 for task crawl_data ('booking_links'; 65)
[2023-09-30T12:30:54.233+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T12:30:54.247+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T12:37:02.226+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T12:37:02.231+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T12:37:02.235+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T12:37:02.236+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T12:37:02.243+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T12:37:02.247+0000] {standard_task_runner.py:57} INFO - Started process 77 to run task
[2023-09-30T12:37:02.249+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmptvjzkjxp']
[2023-09-30T12:37:02.249+0000] {standard_task_runner.py:85} INFO - Job 11: Subtask crawl_data
[2023-09-30T12:37:02.258+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T12:37:02.280+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host 64844768c165
[2023-09-30T12:37:02.288+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T12:37:02.341+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T12:37:02.342+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 20, in extract_data
    booking_links = config['booking_links']
KeyError: 'booking_links'
[2023-09-30T12:37:02.353+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T123702, end_date=20230930T123702
[2023-09-30T12:37:02.362+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 11 for task crawl_data ('booking_links'; 77)
[2023-09-30T12:37:02.381+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T12:37:02.395+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T12:38:15.462+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T12:38:15.468+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T12:38:15.472+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T12:38:15.472+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T12:38:15.479+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T12:38:15.482+0000] {standard_task_runner.py:57} INFO - Started process 83 to run task
[2023-09-30T12:38:15.484+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '13', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpxttkz995']
[2023-09-30T12:38:15.485+0000] {standard_task_runner.py:85} INFO - Job 13: Subtask crawl_data
[2023-09-30T12:38:15.492+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T12:38:15.510+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host 64844768c165
[2023-09-30T12:38:15.515+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T12:38:15.551+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T12:39:50.078+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 23, in extract_data
    conn_tmp = MySqlHook(mysql_conn_id = 'RawTemp').get_sqlalchemy_engine()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/common/sql/hooks/sql.py", line 199, in get_sqlalchemy_engine
    return create_engine(self.get_uri(), **engine_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/common/sql/hooks/sql.py", line 186, in get_uri
    conn = self.get_connection(getattr(self, self.conn_name_attr))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/base.py", line 72, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/connection.py", line 477, in get_connection_from_secrets
    raise AirflowNotFoundException(f"The conn_id `{conn_id}` isn't defined")
airflow.exceptions.AirflowNotFoundException: The conn_id `RawTemp` isn't defined
[2023-09-30T12:39:50.081+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T123815, end_date=20230930T123950
[2023-09-30T12:39:50.090+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 13 for task crawl_data (The conn_id `RawTemp` isn't defined; 83)
[2023-09-30T12:39:50.146+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T12:39:50.161+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T12:43:01.527+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T12:43:01.535+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T12:43:01.542+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T12:43:01.542+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T12:43:01.551+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T12:43:01.558+0000] {standard_task_runner.py:57} INFO - Started process 89 to run task
[2023-09-30T12:43:01.563+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '15', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmp6mdxmzy8']
[2023-09-30T12:43:01.565+0000] {standard_task_runner.py:85} INFO - Job 15: Subtask crawl_data
[2023-09-30T12:43:01.587+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T12:43:01.640+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host 64844768c165
[2023-09-30T12:43:01.668+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T12:43:01.748+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T12:44:32.544+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T12:44:32.552+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 24, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T12:44:32.560+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T124301, end_date=20230930T124432
[2023-09-30T12:44:32.567+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 15 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 89)
[2023-09-30T12:44:32.591+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T12:44:32.610+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T13:51:31.724+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T13:51:31.729+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T13:51:31.734+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T13:51:31.734+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T13:51:31.741+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T13:51:31.744+0000] {standard_task_runner.py:57} INFO - Started process 95 to run task
[2023-09-30T13:51:31.746+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpkjk99idz']
[2023-09-30T13:51:31.746+0000] {standard_task_runner.py:85} INFO - Job 17: Subtask crawl_data
[2023-09-30T13:51:31.755+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T13:51:31.787+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host 64844768c165
[2023-09-30T13:51:31.802+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T13:51:31.878+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T13:53:07.874+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T13:53:07.878+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 22, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T13:53:07.887+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T135131, end_date=20230930T135307
[2023-09-30T13:53:07.905+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 17 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 95)
[2023-09-30T13:53:07.925+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T13:53:07.941+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T14:12:23.387+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T14:12:23.392+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T14:12:23.397+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T14:12:23.397+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T14:12:23.404+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T14:12:23.408+0000] {standard_task_runner.py:57} INFO - Started process 53 to run task
[2023-09-30T14:12:23.410+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpj1u92vhx']
[2023-09-30T14:12:23.410+0000] {standard_task_runner.py:85} INFO - Job 20: Subtask crawl_data
[2023-09-30T14:12:23.419+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T14:12:23.445+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host 29af2c427f7f
[2023-09-30T14:12:23.458+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T14:12:23.505+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T14:13:59.917+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T14:13:59.921+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 22, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T14:13:59.928+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T141223, end_date=20230930T141359
[2023-09-30T14:13:59.937+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 20 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 53)
[2023-09-30T14:13:59.965+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T14:13:59.981+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T14:25:43.671+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T14:25:43.677+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T14:25:43.681+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T14:25:43.682+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T14:25:43.688+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T14:25:43.692+0000] {standard_task_runner.py:57} INFO - Started process 53 to run task
[2023-09-30T14:25:43.694+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '23', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpfisqrs4e']
[2023-09-30T14:25:43.695+0000] {standard_task_runner.py:85} INFO - Job 23: Subtask crawl_data
[2023-09-30T14:25:43.705+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T14:25:43.728+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host d0f75dba0c31
[2023-09-30T14:25:43.737+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T14:25:43.778+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T14:27:19.609+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T14:27:19.610+0000] {logging_mixin.py:151} INFO - conn_tmp Engine(mysql://sltadm:***@host.docker.internal:3306/TEMP?__extra__=%7B%7D)
[2023-09-30T14:27:19.613+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 23, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T14:27:19.621+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T142543, end_date=20230930T142719
[2023-09-30T14:27:19.628+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 23 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 53)
[2023-09-30T14:27:19.661+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T14:27:19.676+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T14:34:05.425+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T14:34:05.430+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T14:34:05.434+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T14:34:05.435+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T14:34:05.441+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T14:34:05.445+0000] {standard_task_runner.py:57} INFO - Started process 53 to run task
[2023-09-30T14:34:05.447+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpntgw2m4p']
[2023-09-30T14:34:05.448+0000] {standard_task_runner.py:85} INFO - Job 26: Subtask crawl_data
[2023-09-30T14:34:05.456+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T14:34:05.479+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host 4c1913fe0f0b
[2023-09-30T14:34:05.493+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T14:34:05.534+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T14:35:36.278+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T14:35:36.278+0000] {logging_mixin.py:151} INFO - conn_tmp Engine(mysql://sltadm:***@host.docker.internal:3306/TEMP?__extra__=%7B%7D)
[2023-09-30T14:35:36.281+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 23, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T14:35:36.289+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T143405, end_date=20230930T143536
[2023-09-30T14:35:36.303+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 26 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 53)
[2023-09-30T14:35:36.335+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T14:35:36.351+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T14:54:12.964+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T14:54:12.975+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T14:54:12.980+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T14:54:12.980+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T14:54:12.987+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T14:54:12.991+0000] {standard_task_runner.py:57} INFO - Started process 53 to run task
[2023-09-30T14:54:12.994+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '30', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpltiptqfg']
[2023-09-30T14:54:12.995+0000] {standard_task_runner.py:85} INFO - Job 30: Subtask crawl_data
[2023-09-30T14:54:13.003+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T14:54:13.024+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T14:54:13.030+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T14:54:13.073+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T14:54:13.078+0000] {logging_mixin.py:151} INFO - len data======= 2
[2023-09-30T14:54:13.083+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T14:54:13.084+0000] {logging_mixin.py:151} INFO - conn_tmp....... Engine(mysql://sltadm:***@host.docker.internal:3306/TEMP?__extra__=%7B%7D)
[2023-09-30T14:54:13.088+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 25, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T14:54:13.101+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T145412, end_date=20230930T145413
[2023-09-30T14:54:13.109+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 30 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 53)
[2023-09-30T14:54:13.126+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T14:54:13.141+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:03:03.791+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:03:03.802+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:03:03.811+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:03:03.812+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:03:03.836+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:03:03.842+0000] {standard_task_runner.py:57} INFO - Started process 59 to run task
[2023-09-30T15:03:03.846+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmp7b2284y1']
[2023-09-30T15:03:03.848+0000] {standard_task_runner.py:85} INFO - Job 32: Subtask crawl_data
[2023-09-30T15:03:03.865+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:03:03.898+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:03:03.910+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:03:03.982+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:03:03.990+0000] {logging_mixin.py:151} INFO - len data======= 2
[2023-09-30T15:03:04.006+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:03:04.008+0000] {logging_mixin.py:151} INFO - conn_tmp....... Engine(mysql://sltadm:***@host.docker.internal:3306/TEMP?__extra__=%7B%7D)
[2023-09-30T15:03:04.009+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 25, in extract_data
    print('conn.....', MySqlHook(mysql_conn_id = config_db['database']).get_conn())
NameError: name 'config_db' is not defined
[2023-09-30T15:03:04.021+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T150303, end_date=20230930T150304
[2023-09-30T15:03:04.031+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 32 for task crawl_data (name 'config_db' is not defined; 59)
[2023-09-30T15:03:04.059+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T15:03:04.079+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:03:58.113+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:03:58.118+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:03:58.122+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:03:58.122+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:03:58.130+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:03:58.134+0000] {standard_task_runner.py:57} INFO - Started process 65 to run task
[2023-09-30T15:03:58.136+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpjblfpdqz']
[2023-09-30T15:03:58.137+0000] {standard_task_runner.py:85} INFO - Job 34: Subtask crawl_data
[2023-09-30T15:03:58.146+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:03:58.167+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:03:58.177+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:03:58.223+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:03:58.228+0000] {logging_mixin.py:151} INFO - len data======= 2
[2023-09-30T15:03:58.234+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:03:58.235+0000] {logging_mixin.py:151} INFO - conn_tmp....... Engine(mysql://sltadm:***@host.docker.internal:3306/TEMP?__extra__=%7B%7D)
[2023-09-30T15:03:58.240+0000] {base.py:73} INFO - Using connection ID 'Booking' for task execution.
[2023-09-30T15:03:58.243+0000] {logging_mixin.py:151} INFO - conn..... <_mysql.connection open to 'host.docker.internal' at 0xaaaafb39e920>
[2023-09-30T15:03:58.251+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 26, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T15:03:58.267+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T150358, end_date=20230930T150358
[2023-09-30T15:03:58.274+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 34 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 65)
[2023-09-30T15:03:58.311+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T15:03:58.327+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:22:37.088+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:22:37.097+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:22:37.105+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:22:37.105+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:22:37.141+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:22:37.146+0000] {standard_task_runner.py:57} INFO - Started process 71 to run task
[2023-09-30T15:22:37.156+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '36', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpe4i4tjfy']
[2023-09-30T15:22:37.157+0000] {standard_task_runner.py:85} INFO - Job 36: Subtask crawl_data
[2023-09-30T15:22:37.183+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:22:37.232+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:22:37.252+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:22:37.412+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:22:37.425+0000] {logging_mixin.py:151} INFO - len data======= 2
[2023-09-30T15:22:37.436+0000] {base.py:73} INFO - Using connection ID 'Booking' for task execution.
[2023-09-30T15:22:37.445+0000] {warnings.py:109} WARNING - /opt/airflow/dags/help_function/ETL_Data.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  data = pd.read_sql('Select * From BookingHotel',conn)

[2023-09-30T15:22:37.493+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:22:37.494+0000] {logging_mixin.py:151} INFO - conn_tmp....... Engine(mysql://sltadm:***@host.docker.internal:3306/TEMP?__extra__=%7B%7D)
[2023-09-30T15:22:37.495+0000] {logging_mixin.py:151} INFO - conn..... <_mysql.connection open to 'host.docker.internal' at 0xaaaafb3a60e0>
[2023-09-30T15:22:37.495+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 28, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T15:22:37.506+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T152237, end_date=20230930T152237
[2023-09-30T15:22:37.513+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 36 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 71)
[2023-09-30T15:22:37.522+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T15:22:37.538+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:25:18.027+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:25:18.032+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:25:18.036+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:25:18.036+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:25:18.043+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:25:18.047+0000] {standard_task_runner.py:57} INFO - Started process 77 to run task
[2023-09-30T15:25:18.049+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '38', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmp8q45bq_5']
[2023-09-30T15:25:18.050+0000] {standard_task_runner.py:85} INFO - Job 38: Subtask crawl_data
[2023-09-30T15:25:18.059+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:25:18.079+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:25:18.087+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:25:18.126+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:25:18.134+0000] {base.py:73} INFO - Using connection ID 'Booking' for task execution.
[2023-09-30T15:25:18.140+0000] {warnings.py:109} WARNING - /opt/airflow/dags/help_function/ETL_Data.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.
  data = pd.read_sql('Select * From BookingHotel',conn)

[2023-09-30T15:25:18.145+0000] {logging_mixin.py:151} INFO - len data======= 0
[2023-09-30T15:25:18.149+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:25:18.150+0000] {logging_mixin.py:151} INFO - conn_tmp....... Engine(mysql://sltadm:***@host.docker.internal:3306/TEMP?__extra__=%7B%7D)
[2023-09-30T15:25:18.150+0000] {logging_mixin.py:151} INFO - conn..... <_mysql.connection open to 'host.docker.internal' at 0xaaaafb3a53a0>
[2023-09-30T15:25:18.151+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 29, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T15:25:18.159+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T152518, end_date=20230930T152518
[2023-09-30T15:25:18.166+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 38 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 77)
[2023-09-30T15:25:18.182+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T15:25:18.197+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:27:51.553+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:27:51.558+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:27:51.563+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:27:51.563+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:27:51.570+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:27:51.574+0000] {standard_task_runner.py:57} INFO - Started process 83 to run task
[2023-09-30T15:27:51.577+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '40', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmp71c_rh89']
[2023-09-30T15:27:51.577+0000] {standard_task_runner.py:85} INFO - Job 40: Subtask crawl_data
[2023-09-30T15:27:51.586+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:27:51.606+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:27:51.614+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:27:51.669+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:27:51.679+0000] {base.py:73} INFO - Using connection ID 'Booking' for task execution.
[2023-09-30T15:27:51.681+0000] {logging_mixin.py:151} INFO - conn_tmp....... Engine(mysql://sltadm:***@host.docker.internal:3306/Booking?__extra__=%7B%7D)
[2023-09-30T15:27:51.684+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 29, in extract_data
    data.to_sql(self.name_raw_table,conn_tmp,if_exists = 'replace',index=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 2878, in to_sql
    return sql.to_sql(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 768, in to_sql
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/__init__.py", line 121, in Connect
    return Connection(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/MySQLdb/connections.py", line 193, in __init__
    super().__init__(*args, **kwargs2)
TypeError: '__extra__' is an invalid keyword argument for connect()
[2023-09-30T15:27:51.695+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T152751, end_date=20230930T152751
[2023-09-30T15:27:51.704+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 40 for task crawl_data ('__extra__' is an invalid keyword argument for connect(); 83)
[2023-09-30T15:27:51.753+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T15:27:51.768+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:32:58.582+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:32:58.588+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:32:58.593+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:32:58.593+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:32:58.600+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:32:58.605+0000] {standard_task_runner.py:57} INFO - Started process 89 to run task
[2023-09-30T15:32:58.610+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpyjttqcc6']
[2023-09-30T15:32:58.611+0000] {standard_task_runner.py:85} INFO - Job 42: Subtask crawl_data
[2023-09-30T15:32:58.622+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:32:58.651+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:32:58.668+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:32:58.749+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:32:58.764+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:32:58.767+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 192, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 209, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/help_function/ETL_Data.py", line 24, in extract_data
    insert_db(data,conn,self.name_raw_table)
NameError: name 'insert_db' is not defined
[2023-09-30T15:32:58.773+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T153258, end_date=20230930T153258
[2023-09-30T15:32:58.783+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 42 for task crawl_data (name 'insert_db' is not defined; 89)
[2023-09-30T15:32:58.826+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2023-09-30T15:32:58.842+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:35:25.246+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:35:25.253+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:35:25.258+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:35:25.258+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:35:25.265+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:35:25.270+0000] {standard_task_runner.py:57} INFO - Started process 95 to run task
[2023-09-30T15:35:25.273+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '44', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpglob2yyf']
[2023-09-30T15:35:25.273+0000] {standard_task_runner.py:85} INFO - Job 44: Subtask crawl_data
[2023-09-30T15:35:25.283+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:35:25.310+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:35:25.318+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:35:25.362+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:35:25.370+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:35:25.387+0000] {logging_mixin.py:151} INFO - Insert error:  (1146, "Table 'temp.raw_data' doesn't exist")
[2023-09-30T15:35:25.388+0000] {python.py:194} INFO - Done. Returned value was: None
[2023-09-30T15:35:25.394+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T153525, end_date=20230930T153525
[2023-09-30T15:35:25.405+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-09-30T15:35:25.424+0000] {taskinstance.py:2784} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:39:25.087+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:39:25.093+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:39:25.098+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:39:25.098+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:39:25.106+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:39:25.110+0000] {standard_task_runner.py:57} INFO - Started process 104 to run task
[2023-09-30T15:39:25.112+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '47', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpvqzvoo_h']
[2023-09-30T15:39:25.112+0000] {standard_task_runner.py:85} INFO - Job 47: Subtask crawl_data
[2023-09-30T15:39:25.121+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:39:25.141+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:39:25.148+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:39:25.194+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:40:59.159+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:40:59.189+0000] {logging_mixin.py:151} INFO - Insert error:  (1146, "Table 'temp.raw_data' doesn't exist")
[2023-09-30T15:40:59.190+0000] {python.py:194} INFO - Done. Returned value was: None
[2023-09-30T15:40:59.197+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T153925, end_date=20230930T154059
[2023-09-30T15:40:59.248+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-09-30T15:40:59.273+0000] {taskinstance.py:2784} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:42:44.877+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:42:44.882+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:42:44.886+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:42:44.887+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:42:44.893+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:42:44.897+0000] {standard_task_runner.py:57} INFO - Started process 113 to run task
[2023-09-30T15:42:44.899+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmp6yhissjq']
[2023-09-30T15:42:44.900+0000] {standard_task_runner.py:85} INFO - Job 50: Subtask crawl_data
[2023-09-30T15:42:44.908+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:42:44.927+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:42:44.933+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:42:44.975+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:44:18.340+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:44:18.358+0000] {logging_mixin.py:151} INFO - Insert error:  (1146, "Table 'temp.raw_data' doesn't exist")
[2023-09-30T15:44:18.359+0000] {python.py:194} INFO - Done. Returned value was: None
[2023-09-30T15:44:18.365+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T154244, end_date=20230930T154418
[2023-09-30T15:44:18.406+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-09-30T15:44:18.426+0000] {taskinstance.py:2784} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-09-30T15:47:02.746+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:47:02.752+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:47:02.756+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [queued]>
[2023-09-30T15:47:02.756+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2023-09-30T15:47:02.762+0000] {taskinstance.py:1382} INFO - Executing <Task(PythonOperator): crawl_data> on 2023-09-30 00:00:00+00:00
[2023-09-30T15:47:02.766+0000] {standard_task_runner.py:57} INFO - Started process 125 to run task
[2023-09-30T15:47:02.768+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'ETL', 'crawl_data', 'scheduled__2023-09-30T00:00:00+00:00', '--job-id', '54', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpcmpzp_t3']
[2023-09-30T15:47:02.768+0000] {standard_task_runner.py:85} INFO - Job 54: Subtask crawl_data
[2023-09-30T15:47:02.777+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2023-09-30T15:47:02.797+0000] {task_command.py:415} INFO - Running <TaskInstance: ETL.crawl_data scheduled__2023-09-30T00:00:00+00:00 [running]> on host e8049923f551
[2023-09-30T15:47:02.804+0000] {warnings.py:109} WARNING - /home/airflow/.local/lib/python3.8/site-packages/airflow/utils/sqlalchemy.py:124: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  return not conf.get("database", "sql_alchemy_conn").startswith("mssql")

[2023-09-30T15:47:02.845+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='ETL' AIRFLOW_CTX_TASK_ID='crawl_data' AIRFLOW_CTX_EXECUTION_DATE='2023-09-30T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-09-30T00:00:00+00:00'
[2023-09-30T15:48:38.385+0000] {base.py:73} INFO - Using connection ID 'RawTemp' for task execution.
[2023-09-30T15:48:38.395+0000] {logging_mixin.py:151} INFO - Insert error:  (1146, "Table 'temp.raw_data' doesn't exist")
[2023-09-30T15:48:38.395+0000] {python.py:194} INFO - Done. Returned value was: None
[2023-09-30T15:48:38.533+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=ETL, task_id=crawl_data, execution_date=20230930T000000, start_date=20230930T154702, end_date=20230930T154838
[2023-09-30T15:48:38.589+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2023-09-30T15:48:38.616+0000] {taskinstance.py:2784} INFO - 1 downstream tasks scheduled from follow-on schedule check
